default:
  image: python:3-slim
variables:
  DOCKER_TLS_CERTDIR: ""
  APP_VERSION: "beta"
  OS_AUTH_TYPE: v3applicationcredential
  OS_AUTH_URL: https://hdf-cloud.fz-juelich.de:5000
  OS_IDENTITY_API_VERSION: 3
  OS_REGION_NAME: "HDFCloud"
  OS_INTERFACE: public
  PRODUCTION_URL: https://datacatalog.fz-juelich.de/
  PRODUCTION_DOMAIN: datacatalog.fz-juelich.de
  VOLUME_ID: 07a93071-5be7-4cc0-8ff3-cb34e7ed2b80
  PRODUCTION_IP: 134.94.199.59
  TESTING_URL: https://zam10036.zam.kfa-juelich.de/
  TESTING_DOMAIN: zam10036.zam.kfa-juelich.de
  TESTING_IP: 134.94.199.36
  ROLLBACK_COMMIT_TAG: e2c528fcc617dfc01a8e6b3a8ffcddc4abb1f67b

# before script copied from gitlab docs
before_script:
  - 'command -v ssh-agent >/dev/null || ( apt-get update -y && apt-get install openssh-client -y )'
  - eval $(ssh-agent -s)
  - echo "$SSH_PRIVATE_KEY" | tr -d '\r' | ssh-add -
  - mkdir -p ~/.ssh
  - chmod 700 ~/.ssh

stages:
  - test
  - build
  - publish
  - deploy
  - test-deployment
  - cleanup

test:
  stage: test
  script: 
   - pip install -r testing_requirements.txt
   - nosetests --with-coverage --cover-package=apiserver --cover-xml
  artifacts:
    reports:
      cobertura: coverage.xml

light-deploy-testing:
  stage: deploy 
  # only run when master is updated, unless the pipeline was triggered via the web UI
  only:
    - master
  except:
    - tags
    - web
  environment: Testing
  script:
    - ssh -oStrictHostKeyChecking=accept-new apiserver@$TESTING_DOMAIN "cd /home/apiserver/datacatalog && sudo git pull --all && sudo git checkout -f $CI_COMMIT_TAG"
    - ssh -oStrictHostKeyChecking=accept-new apiserver@$TESTING_DOMAIN "sudo /home/apiserver/datacatalog/deploy_scripts/deployment.sh /home/apiserver/datacatalog $TESTING_URL $TESTING_DOMAIN"

light-deploy-production:
  stage: deploy 
  # only run when master is updated, unless the pipeline was triggered via the web UI
  only:
    - tags
  except:
    - web
  tags: [stable]
  environment: Production
  script:
    - ssh -oStrictHostKeyChecking=accept-new apiserver@$PRODUCTION_DOMAIN "cd /home/apiserver/datacatalog && sudo git pull --all && sudo git checkout -f $CI_COMMIT_TAG"
    - ssh -oStrictHostKeyChecking=accept-new apiserver@$PRODUCTION_DOMAIN "sudo /home/apiserver/datacatalog/deploy_scripts/deployment.sh /home/apiserver/datacatalog $PRODUCTION_URL $PRODUCTION_DOMAIN"

full-deploy-production:
  stage: deploy
  # only run when stable tag is assigned and the pipeline is triggered in the web UI
  only:
    - tags && web
  tags: [stable]
  environment: Production
  script:
    - echo "Starting the full production deployment."
    - pip install python-openstackclient
    - OLD_ID=`openstack server show production-deployment -f value -c id`
    # TODO rename old instance, so that we can find it in cleanup task
    # TODO get and locally store zip of old certificate-docker-volume
    # don't create snapshot copy of old instance, we keep the old instance alive as long as possible
    # add should work without removing first- openstack server remove floating ip $OLD_ID $PRODUCTION_IP
    - openstack server remove volume $OLD_ID $VOLUME_ID
    - INSTANCE_ID=`openstack server create -f value -c id --prefix IMAGE_ --flavor s2 --image 149a65b5-aeb8-499f-aaa6-ec966bd28dd6 --user-data deploy_scripts/cloudinit.yml --security-group ssh --security-group www --security-group https production-deployment`
    - while [ "`openstack server show $INSTANCE_ID -c addresses -f value`" = "{}" ]; do sleep 5; done # wait until an address is available to attach the floating ip
    - openstack server add floating ip $INSTANCE_ID $PRODUCTION_IP
    # TODO move local zip of certificate-docker-volume to server once startup is complete
    - openstack server add volume $INSTANCE_ID $VOLUME_ID
    - ssh -oStrictHostKeyChecking=accept-new apiserver@$PRODUCTION_DOMAIN "mount /dev/vdb1 /app/mnt"
    - ssh -oStrictHostKeyChecking=accept-new apiserver@$PRODUCTION_DOMAIN "until [ -e /finished_cloudinit ]; do sleep 5; done" # wait until cloudinit script is complete - this should also mean that the server has started TODO check this

full-deploy-testing:
  stage: deploy 
  # only run when master is updated and the pipeline is triggered in the web UI
  only:
    - master && web
  except:
    - tags
  environment: Testing
  script:
    - echo "Starting the full testing deployment."
    - sed -i 's_datacatalog.fz_zam10036.zam.kfa_g' deploy_scripts/cloudinit.yml
    - pip install python-openstackclient
    - OLD_ID=`openstack server show testing-deployment -f value -c id`
    # TODO rename old instance, so that we can find it in cleanup task
    # TODO get and locally store zip of old certificate-docker-volume
    # add should work without removing first- openstack server remove floating ip $OLD_ID $TESTING_IP
    - INSTANCE_ID=`openstack server create -f value -c id --prefix IMAGE_ --flavor s1 --image 149a65b5-aeb8-499f-aaa6-ec966bd28dd6 --user-data deploy_scripts/cloudinit.yml --security-group ssh --security-group www --security-group https testing-deployment`
    - while [ "`openstack server show $INSTANCE_ID -c addresses -f value`" = "{}" ]; do sleep 5; done # wait until an address is available to attach the floating ip
    - openstack server add floating ip $INSTANCE_ID $TESTING_IP
    # TODO move local zip of certificate-docker-volume to server once startup is complete
    - ssh -oStrictHostKeyChecking=accept-new apiserver@$TESTING_DOMAIN "until [ -e /finished_cloudinit ]; do sleep 5; done" # wait until cloudinit script is complete - this should also mean that the server has started TODO check this

  
cleanup-failed-full-deployment:
  # check if there is an old prod or test instance, assign respective ip to it, re-attach volume, delete new instance, rename old instance
  # if there is none, this is a failed light-deployment, which is handled by another job
  # this does not guarantee a successful rollback, but unless the old instance was faulty, this should work
  stage: cleanup
  when: on_failure
  only:
    - web
  script:
    - echo "This is the cleanup for the full-redeployment of the testing or production servers"
    - echo "if this job is reached, some earlier job had to have failed, this will return to the previous instance (if available)"
    - echo "A successfull cleanup can not be guaranteed, depending on the failure reason"
    # TODO check which old instance is present. (either test-old or production-old); store instance id in var test_id and prod_id
    # TODO if test_id is set, rollback test ip address, rename test instance and delete new instance
    # TODO if prod_id is set, rollback prod ip, remove new instance, attach volume to old, remname prod instance
    # gitlab should automatically alert the devs about this failure

cleanup-successful-full-deployment:
  # check if there is an old prod or test instance, and delete it if present
  stage: cleanup
  when: on_success
  only:
    - web
  script:
    - echo "This is the cleanup for the full-redeployment of the testing or production servers"
    - echo "if this job is reached, some earlier job had to have failed, this will return to the previous instance (if available)"
    - echo "A successfull cleanup can not be guaranteed, depending on the failure reason"
    # TODO check which old instance is present. (eithger test-old or production-old); store instance id in var test_id and prod_id
    # TODO if test_id is set, delete it
    # TODO if prod_id is set, delete it

cleanup-failed-light-test-deployment:
  # if there is a failure with the light deployments, this tries to git checkout an earlier version and rollback to that.
  stage: cleanup
  when: on_failure
  only:
    - master
  except:
    - tags
    - web
  script:
    - echo "This is the cleanup for the light-redeployment of the testing servers"
    - echo "if this job is reached, some earlier job had to have failed, this will return to a previous commit"
    - echo "A successfull cleanup can not be guaranteed, depending on the failure reason"
    - COMMIT_TAG="$ROLLBACK_COMMIT_TAG" # a stable base version here, shpuld be updated once a release (i.e. a stable-XX tag) has been proven stable in pracice
    - ssh -oStrictHostKeyChecking=accept-new apiserver@$TESTING_DOMAIN "cd /home/apiserver/datacatalog && sudo git pull --all && sudo git checkout -f $COMMIT_TAG"
    - ssh -oStrictHostKeyChecking=accept-new apiserver@$TESTING_DOMAIN "sudo /home/apiserver/datacatalog/deploy_scripts/deployment.sh /home/apiserver/datacatalog $TESTING_URL $TESTING_DOMAIN"


cleanup-failed-light-production-deployment:
  # if there is a failure with the light deployments, this tries to git checkout an earlier version and rollback to that.
  stage: cleanup
  when: on_failure
  only:
    - tags
  except:
    - web
  tags: [stable]
  script:
    - echo "This is the cleanup for the light-redeployment of the production servers"
    - echo "if this job is reached, some earlier job had to have failed, this will return to a previous commit"
    - echo "A successfull cleanup can not be guaranteed, depending on the failure reason"
    - COMMIT_TAG="$ROLLBACK_COMMIT_TAG" # some stable base version here, should be updated if an important commit has been proven stable
    - ssh -oStrictHostKeyChecking=accept-new apiserver@$PRODUCTION_DOMAIN "cd /home/apiserver/datacatalog && sudo git pull --all && sudo git checkout -f $COMMIT_TAG"
    - ssh -oStrictHostKeyChecking=accept-new apiserver@$PRODUCTION_DOMAIN "sudo /home/apiserver/datacatalog/deploy_scripts/deployment.sh /home/apiserver/datacatalog $PRODUCTION_URL $PRODUCTION_DOMAIN"
  
test-testing:
  cache: {}
  stage: test-deployment 
  only:
    - master
  except:
    - tags
  variables:
  script:
    - apt update && apt -y install curl
    - echo "For now, this will be a basic health check i.e. GET / and check for 2xx code."
    - 'curl -f -H "Accept: application/json" $TESTING_URL'

test-production:
  cache: {}
  stage: test-deployment 
  only:
    - tags
  tags: [stable]
  script:
    - apt update && apt -y install curl
    - echo "For now, this will be a basic health check i.e. GET / and check for 2xx code."
    - 'curl -f -H "Accept: application/json" $PRODUCTION_URL' 

publishgit-do:
  image: python:3-slim
  stage: publish
  only:
    - tags
  tags: [stable]
  script:
    - apt-get update 
    - apt-get install -y git
    - (git remote -v | grep gith) || git remote add gith "https://${GITHUB_USER}:${GITHUB_TOKEN}@github.com/eflows4hpc/datacatalog.git"
    - git remote -v
    - git show-ref
    - export
    - git push gith $CI_COMMIT_REF_NAME


# This is an automatic push of the docker image into gitLab container repository
transfer_image:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  variables:
    IMAGE_COMMIT_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA
    IMAGE_MASTER_TAG: $CI_REGISTRY_IMAGE:master
  script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker build --no-cache=true --pull -f ./apiserver/Dockerfile -t $IMAGE_COMMIT_TAG .
    - docker push $IMAGE_COMMIT_TAG
    - docker tag $IMAGE_COMMIT_TAG $IMAGE_MASTER_TAG
    - docker push $IMAGE_MASTER_TAG

tag_release:
  stage: publish
  image: docker:latest
  services:
    - docker:dind
  variables:
    IMAGE_LATEST_TAG: $CI_REGISTRY_IMAGE:latest
    IMAGE_STABLE_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_TAG
    IMAGE_COMMIT_TAG: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA
  only:
    - tags
  tags: [stable]
  script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker pull $IMAGE_COMMIT_TAG
    - docker tag $IMAGE_COMMIT_TAG $IMAGE_STABLE_TAG
    - docker tag $IMAGE_COMMIT_TAG $IMAGE_LATEST_TAG
    - docker push $IMAGE_STABLE_TAG
    - docker push $IMAGE_LATEST_TAG
